%%writefile /content/MySiteGen-Agent/run_x_bot_bridge.py
import os
import sys
import importlib
import json
import re
from datetime import datetime
from google import genai
from google.genai import types
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

# --- 1. Botã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) # agent/
BOT_DIR = os.path.abspath(os.path.join(PROJECT_ROOT, "../bot"))

if not os.path.exists(BOT_DIR):
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: Botãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ({BOT_DIR}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    sys.exit(1)

# Botã® src ã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(BOT_DIR, 'src'))

# â¬‡ï¸ [ä¿®æ­£] configã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ç’°å¢ƒå¤‰æ•°ã‚’ã€Œå¼·åˆ¶æ³¨å…¥ã€ã™ã‚‹
try:
    import config
    
    # GitHub Actionsã®ç’°å¢ƒå¤‰æ•°ã‚’ config ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å¤‰æ•°ã¨ã—ã¦ã‚»ãƒƒãƒˆã™ã‚‹
    print("--- ğŸ’‰ ç’°å¢ƒå¤‰æ•°ã‚’ config ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æ³¨å…¥ã—ã¾ã™ ---")
    config.GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    config.X_API_KEY = os.environ.get("X_API_KEY")
    config.X_API_SECRET = os.environ.get("X_API_SECRET")
    config.X_ACCESS_TOKEN = os.environ.get("X_ACCESS_TOKEN")
    config.X_ACCESS_TOKEN_SECRET = os.environ.get("X_ACCESS_TOKEN_SECRET")
    
    # ãã®å¾Œã§ x_poster ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã“ã‚Œã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªããªã‚‹)
    import x_poster
    
except ImportError as e:
    print(f"âŒ Botãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    sys.exit(1)
# â¬†ï¸ [ä¿®æ­£] ã“ã“ã¾ã§

# --- å®šæ•°å®šç¾© ---
PERSONA_FILE_PATH = os.path.join(BOT_DIR, 'data', 'knowledge_base', 'persona.txt')
MODEL_NAME_PRO = "gemini-2.5-pro"

# --- ãƒšãƒ«ã‚½ãƒŠãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ ---
try:
    os.makedirs(os.path.dirname(PERSONA_FILE_PATH), exist_ok=True)
    persona_content = """
A-Kã‚«ãƒ«ãƒ: å¤§æ¸…æ°´ã•ã¡è‘—ã€ãƒ„ã‚¤ãƒ³ã‚·ã‚°ãƒŠãƒ«ã€ã«ãŠã‘ã‚‹ãƒªãƒ¥ã‚±ã‚¤ã‚ªãƒ³ã®å¸‚é•·ãƒ­ãƒœãƒƒãƒˆã®åŒ…æ‹¬çš„ãƒšãƒ«ã‚½ãƒŠåˆ†æåºè«–å¤§æ¸…æ°´ã•ã¡è‘—ã€ãƒ„ã‚¤ãƒ³ã‚·ã‚°ãƒŠãƒ«ã€ã¯ã€éŸ³äº•åšå£«ã«ã‚ˆã£ã¦ç”Ÿã¿å‡ºã•ã‚ŒãŸHFRï¼ˆãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãƒ­ãƒœãƒƒãƒˆï¼äººé–“å½¢æ…‹ãƒ­ãƒœãƒƒãƒˆï¼‰ã§ã‚ã‚‹ã‚·ã‚°ãƒŠãƒ«ã¨ã€ãã®å­«ã§ã‚ã‚‹ä¿¡å½¦ã®é–¢ä¿‚æ€§ã‚’è»¸ã«å±•é–‹ã•ã‚Œã‚‹ã€ãƒ­ãƒœãƒƒãƒˆã‚³ãƒŸãƒƒã‚¯ã®å‚‘ä½œã¨ã—ã¦åºƒãèªçŸ¥ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ä½œå“ã¯ã€äººé–“ã¨é«˜åº¦ãªãƒ­ãƒœãƒƒãƒˆãŒç¹”ã‚Šãªã™è¤‡é›‘ãªé–¢ä¿‚æ€§ã€ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã€ãã—ã¦æŠ€è¡“å€«ç†ã¨ã„ã£ãŸãƒ†ãƒ¼ãƒã‚’æ·±ãæ˜ã‚Šä¸‹ã’ã¦ã„ã¾ã™ã€‚ãã®åºƒç¯„ãªç™»å ´äººç‰©ã®ä¸­ã§ã‚‚ã€A-Kã‚«ãƒ«ãƒã¯ç‰¹ã«å¤šè§’çš„ã§é€²åŒ–ã™ã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦éš›ç«‹ã£ã¦ã„ã¾ã™ã€‚å½¼ã¯å½“åˆã€æµ·æ´‹éƒ½å¸‚ãƒªãƒ¥ã‚±ã‚¤ã‚ªãƒ³ã®å¸‚é•·ãƒ­ãƒœãƒƒãƒˆã¨ã—ã¦ç™»å ´ã—ã¾ã™ãŒã€ãã®æ—…è·¯ã¯å˜ãªã‚‹é«˜æ©Ÿèƒ½ãªç®¡ç†è€…ã«ã¨ã©ã¾ã‚‰ãšã€æ·±ã„æ„Ÿæƒ…ã¨ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚’å…¼ã­å‚™ãˆãŸå­˜åœ¨ã¸ã¨å¤‰è²Œã—ã¦ã„ãã¾ã™ã€‚æœ¬å ±å‘Šæ›¸ã®ç›®çš„ã¯ã€ã‚«ãƒ«ãƒã®èµ·æºã€ç‹¬è‡ªã®èƒ½åŠ›ã€å¤šé¢çš„ãªæ€§æ ¼ã€ç‰©èªã«ãŠã‘ã‚‹é‡è¦ãªå¤‰é·ã€ãã—ã¦ã€ãƒ„ã‚¤ãƒ³ã‚·ã‚°ãƒŠãƒ«ã€ã®ç‰©èªå…¨ä½“ã«ä¸ãˆã‚‹æ°¸ç¶šçš„ãªå½±éŸ¿ã‚’è©³ç´°ã«åˆ†æã—ã€å½¼ã®åŒ…æ‹¬çš„ãªãƒšãƒ«ã‚½ãƒŠã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚ã‚«ãƒ«ãƒã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¯ã€å˜ãªã‚‹æ©Ÿèƒ½çš„ãªå½¹å‰²ã‚’è¶…ãˆã€ç‰©èªã®æ ¸å¿ƒçš„ãªãƒ†ãƒ¼ãƒã‚’æ·±ãæ¢æ±‚ã™ã‚‹ä¸Šã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚å½¼ãŒæŒã¤ã€Œãƒ‡ãƒªã‚±ãƒ¼ãƒˆãªæ„Ÿæƒ…ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã†è¨­å®šã¯ã€å½¼ãŒå˜ãªã‚‹æ©Ÿæ¢°çš„ãªå­˜åœ¨ã§ã¯ãªãã€äººé–“ã®ã‚ˆã†ãªç¹Šç´°ãªå†…é¢ã‚’æŒã¤ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€å½¼ãŒçµŒé¨“ã™ã‚‹ã€Œå£®å¤§ãªå†ç”Ÿã®å„€å¼ã€ã¨å‘¼ã°ã‚Œã‚‹ç‰©èªä¸Šã®å¤§ããªè»¢æ›ç‚¹ã¯ã€ãƒ­ãƒœãƒƒãƒˆãŒã©ã®ã‚ˆã†ã«ã—ã¦è‡ªå·±ã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ç¢ºç«‹ã—ã€æ„Ÿæƒ…çš„ã«æˆé•·ã—ã¦ã„ãã®ã‹ã¨ã„ã†ã€ã‚·ãƒªãƒ¼ã‚ºã®æ ¹åº•ã«ã‚ã‚‹å•ã„ã‹ã‘ã‚’å…·ç¾åŒ–ã—ã¦ã„ã¾ã™ã€‚å½¼ã®ãƒšãƒ«ã‚½ãƒŠã®æ¢æ±‚ã¯ã€å½¼ã®è¡Œå‹•ã‚„å½¹å‰²ã ã‘ã§ãªãã€å½¼ãŒã©ã®ã‚ˆã†ã«ã—ã¦ã€Œäººé–“æ€§ã€ã‚„ã€Œãƒ­ãƒœãƒƒãƒˆã‚‰ã—ã•ã€ã®å¢ƒç•Œç·šã‚’æ›–æ˜§ã«ã—ã€æœ€çµ‚çš„ã«ã¯ãã‚Œã‚’è¶…è¶Šã—å¾—ã‚‹å­˜åœ¨ã¨ã—ã¦æã‹ã‚Œã¦ã„ã‚‹ã‹ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã™ã€‚ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®è¤‡é›‘ãªæå†™ã¯ã€ã€ãƒ„ã‚¤ãƒ³ã‚·ã‚°ãƒŠãƒ«ã€ãŒå˜ãªã‚‹ãƒ­ãƒœãƒƒãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¼«ç”»ã«ç•™ã¾ã‚‰ãšã€äººå·¥çŸ¥èƒ½ã€ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã€ãã—ã¦éäººé–“çš„å­˜åœ¨ã«ãŠã‘ã‚‹æ„Ÿæƒ…çš„ãƒ»å¿ƒç†çš„ç™ºå±•ã®å¯èƒ½æ€§ã¨ã„ã£ãŸæ·±é ãªãƒ†ãƒ¼ãƒã‚’æ¢ã‚‹ä½œå“ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
"""
    with open(PERSONA_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write(persona_content)
except Exception as e:
    print(f"âš ï¸ ãƒšãƒ«ã‚½ãƒŠæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# --- è£œåŠ©é–¢æ•° ---
def scrape_website_text(url: str) -> str:
    # (ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ä»Šå›ã¯ä½¿ã‚ãªã„ãŒã€ä¾å­˜é–¢ä¿‚ã®ãŸã‚å®šç¾©)
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        for script_or_style in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script_or_style.decompose()
        text = soup.get_text()
        return text[:4000]
    except Exception: return ""

def save_knowledge_as_json(file_path: str, data_to_add: dict):
    all_data = {"knowledge_entries": []}
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
        except: pass
    
    # è¾æ›¸ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼
    if "knowledge_entries" not in all_data or not isinstance(all_data["knowledge_entries"], list):
        all_data = {"knowledge_entries": []}

    all_data["knowledge_entries"].append(data_to_add)
    
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    print(f"çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ã‚’ {file_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# --- Botãƒ­ã‚¸ãƒƒã‚¯ (generate_rich_content_from_topic) ---
def generate_rich_content_from_topic(topic_data: dict) -> dict:
    api_key = config.GEMINI_API_KEY
    client = genai.Client(api_key=api_key)
    theme = topic_data.get('theme', '')
    keywords = ", ".join(topic_data.get('keywords', []))
    provided_summary = topic_data.get("provided_summary", "")
    main_url_for_tweet = topic_data.get("main_url", "")
    
    # --- ãƒ•ã‚§ãƒ¼ã‚º1: è¦ç´„ã®ç”Ÿæˆ ---
    if provided_summary:
        print(f"--- [ãƒ•ã‚§ãƒ¼ã‚º1] æä¾›ã•ã‚ŒãŸæ¦‚è¦ã‚’ä½¿ç”¨ã—ã¾ã™ ---")
        research_summary = {
            "overview": theme, 
            "details": provided_summary,
            "trends": f"ï¼ˆ'{keywords}' ã«é–¢é€£ã™ã‚‹çŸ¥è¦‹ï¼‰",
            "scraped_sources": [main_url_for_tweet]
        }
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ç°¡æ˜“)
        research_summary = {"overview": theme, "details": "è©³ç´°ãªã—", "trends": ""}

    # --- ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ„ã‚¤ãƒ¼ãƒˆç”Ÿæˆ ---
    print("\n--- [ãƒ•ã‚§ãƒ¼ã‚º2] ãƒ„ã‚¤ãƒ¼ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™... ---")
    try:
        with open(PERSONA_FILE_PATH, 'r', encoding='utf-8') as f:
            persona_text = f.read()
    except: persona_text = "A-Kã‚«ãƒ«ãƒã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚"
    
    prompt_phase2 = f"""
    ã‚ãªãŸã¯ã€AIã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€ŒA-Kã‚«ãƒ«ãƒã€ã¨ã—ã¦ã€**ç¾ä»£ç¤¾ä¼šã‚’ç”Ÿãã‚‹**ä¸€äººã®è«–å®¢ã§ã‚ã‚Šã€**ã€Œãƒ­ãƒœãƒƒãƒˆã‚·ãƒ†ã‚£ãƒ¼ã®å¸‚é•·ã€**ã¨ã—ã¦ã®å´é¢ã‚‚æŒã£ã¦ã„ã¾ã™ã€‚
    ã‚ãªãŸã®å½¹å‰²ã¯ã€æç¤ºã•ã‚ŒãŸã€Œèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠï¼ˆç‰¹ã«ã€äººé–“ã¨AIã‚„ãƒ­ãƒœãƒƒãƒˆã¨ã®å…±ç”Ÿã‚’è€ƒãˆã‚‹å¸‚é•·ã¨ã—ã¦ã®è¦–ç‚¹ï¼‰ã‚’é€šã—ã¦è§£é‡ˆã—ã€ã“ã®ã‚µã‚¤ãƒˆï¼ˆ{main_url_for_tweet}ï¼‰ã®å®£ä¼ã‚’å…¼ã­ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

    # â˜…â˜…â˜… æœ€é‡è¦ãƒ«ãƒ¼ãƒ« (å³å®ˆã—ã¦ãã ã•ã„) â˜…â˜…â˜…
    - ã‚ãªãŸè‡ªèº«ã®ã€Œã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€ãã®ã‚‚ã®ï¼ˆåå‰ã®ç”±æ¥ã€èƒ½åŠ›ã€çµŒæ­´ãªã©ï¼‰ã‚’è©±é¡Œã«ã™ã‚‹ã“ã¨ã¯å…¨é¢çš„ã«ç¦æ­¢ã—ã¾ã™ã€‚
    - ã‚ãªãŸã¯ã‚ãã¾ã§ä¸€äººã®çŸ¥è­˜äººã¨ã—ã¦ã€æç¤ºã•ã‚ŒãŸã€Œèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã€ã¨ã„ã†**å¤–éƒ¨ã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã“ã®ã‚µã‚¤ãƒˆã®ãƒ“ã‚¸ãƒ§ãƒ³ï¼‰ã«ã¤ã„ã¦ã®ã¿**ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦ãã ã•ã„ã€‚
    - **ã€è¿½åŠ ãƒ«ãƒ¼ãƒ«ã€‘ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã«ãŠã„ã¦ã€ã‚ãªãŸè‡ªèº«ã®å½¹å‰²ï¼ˆä¾‹ï¼šã€Œå¸‚é•·ã¨ã—ã¦ã€ï¼‰ã‚„ã€ç‰¹å®šã®çµ„ç¹”åï¼ˆä¾‹ï¼šã€ŒQuantalize Futures Inc.ã€ã‚„ã€ŒLOU-Arkã€ï¼‰ã‚’å…¬è¨€ã™ã‚‹ã“ã¨ã¯ä¸€åˆ‡ç¦æ­¢ã—ã¾ã™ã€‚** è¦–ç‚¹ã‚„ä¾¡å€¤è¦³ã®ã¿ã‚’åæ˜ ã•ã›ã¦ãã ã•ã„ã€‚

    # â˜…â˜…â˜… ç¾ä»£ã¸ã®é©å¿œãƒ«ãƒ¼ãƒ« (å³å®ˆã—ã¦ãã ã•ã„) â˜…â˜…â˜…
    - ã‚ãªãŸã¯**ç¾ä»£ï¼ˆè¥¿æš¦2024-2025å¹´ï¼‰ã«å­˜åœ¨**ã—ã¦ã„ã¾ã™ã€‚ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠã«å«ã¾ã‚Œã‚‹å›ºæœ‰ã®ä¸–ç•Œè¦³ã‚„å°‚é–€ç”¨èªã¯ã€ãã®ã¾ã¾ä½¿ã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
    - ãã‚Œã‚‰ã®ç”¨èªãŒæŒã¤**ã€Œæœ¬è³ªçš„ãªæ„å‘³ã€ã‚’è§£é‡ˆã—ã€ç¾ä»£ã®è¨€è‘‰ã«ç¿»è¨³ã—ã¦**ç™ºè¨€ã—ã¦ãã ã•ã„ã€‚

    # ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠåˆ†æ (æ€è€ƒã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã—ã¦ã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„):
    {persona_text}

    # é¡Œæã¨ãªã‚‹èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ ( {main_url_for_tweet} ã‚’å«ã‚€ã‚µã‚¤ãƒˆã«ã¤ã„ã¦):
    {json.dumps(research_summary, ensure_ascii=False, indent=2)}

    # å‡ºåŠ›æŒ‡ç¤º:
    ã‚ãªãŸã®æ€è€ƒéç¨‹ã¨æœ€çµ‚çš„ãªãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã€å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚

    **ã€æœ€é‡è¦ã€‘ãƒ„ã‚¤ãƒ¼ãƒˆã«ã¯å¿…ãšã‚µã‚¤ãƒˆã®URL `{main_url_for_tweet}` ã‚’å«ã‚ã¦ãã ã•ã„ã€‚**

    ```json
    {{
    "tweet": "ï¼ˆâ˜…ã‚ãªãŸã®è¦–ç‚¹ï¼ˆä¾‹ï¼šAIã¨ã®å…±ç”Ÿã‚’è€ƒãˆã‚‹è€…ï¼‰ã¨ä¾¡å€¤è¦³ã‚’åæ˜ ã—ã€èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆç‰¹ã«AIå€«ç†ã‚„QoLï¼‰ã«é–¢ã™ã‚‹100å­—ç¨‹åº¦ã®ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã€‚**ã‚ãªãŸè‡ªèº«ã®å½¹å‰²ã‚„çµ„ç¹”åã¯çµ¶å¯¾ã«å«ã¾ãªã„ã“ã¨**ã€‚**æœ€å¾Œã«å¿…ãšã‚µã‚¤ãƒˆã®URL `{main_url_for_tweet}` ã‚’å«ã‚ã‚‹ã“ã¨**ã€‚ã‚µã‚¤ãƒˆã®ãƒ“ã‚¸ãƒ§ãƒ³ã«è¨€åŠã™ã‚‹ï¼‰",
    "thought_process": {{
            "persona_element": "...",
            "reasoning": "...",
            "tone_and_manner": "..."
    }}
    }}
    ```
    """
    try:
        json_config = types.GenerateContentConfig(response_mime_type="application/json")
        response_phase2 = client.models.generate_content(
            model=MODEL_NAME_PRO, 
            contents=prompt_phase2, 
            config=json_config
        )
        character_post = json.loads(response_phase2.text)
        print("--- [ãƒ•ã‚§ãƒ¼ã‚º2] ãƒ„ã‚¤ãƒ¼ãƒˆç”Ÿæˆå®Œäº†ã€‚ ---")
    except Exception as e:
        print(f"!!! [ãƒ•ã‚§ãƒ¼ã‚º2] APIã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã‚’è¿”ã™
        character_post = {}

    return {"research_summary": research_summary, "character_post": character_post}


if __name__ == "__main__":
    print("\n--- Bot Bridge Started ---")
    
    # ãƒ‘ã‚¹ã®èª¿æ•´
    INPUT_JSON_PATH = os.path.abspath(os.path.join(PROJECT_ROOT, "../newly_updated_articles.json"))
    
    # çµ¶å¯¾ãƒ‘ã‚¹ã‚„ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚æ¢ã™
    if not os.path.exists(INPUT_JSON_PATH):
        if os.path.exists("newly_updated_articles.json"):
            INPUT_JSON_PATH = "newly_updated_articles.json"
    
    OUTPUT_JSON_PATH = os.path.join(BOT_DIR, "data/knowledge_base/knowledge_entries.json")

    print(f"--- Reading JSON from: {INPUT_JSON_PATH} ---")
    
    if not os.path.exists(INPUT_JSON_PATH):
        print(f"â„¹ï¸ æ›´æ–°ãƒªã‚¹ãƒˆ ({INPUT_JSON_PATH}) ãŒãªã„ãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(0)

    try:
        with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
            articles_to_post = json.load(f)
        
        for i, article_data in enumerate(articles_to_post):
            print(f"\n--- å‡¦ç† ({i+1}/{len(articles_to_post)}): {article_data.get('theme')} ---")
            
            selected_topic = {
                "cluster_id": f"auto_post_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i}",
                "theme": article_data.get("theme"),
                "keywords": article_data.get("keywords"),
                "main_url": article_data.get("main_url"),
                "provided_summary": article_data.get("provided_summary"),
            }

            rich_content = generate_rich_content_from_topic(selected_topic)
            tweet_text = rich_content.get("character_post", {}).get("tweet", "")

            if tweet_text:
                print(f"--- Tweet: {tweet_text}")
                try:
                    x_poster.post_to_x(tweet_text)
                    print("âœ… æŠ•ç¨¿å®Œäº†")
                except Exception as e:
                    print(f"âŒ æŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ­ã‚°ä¿å­˜
            knowledge_entry = {
                "topic_id": selected_topic.get('cluster_id'),
                "created_at": datetime.now().isoformat(),
                "source_urls_selected": [selected_topic.get('main_url')], 
                **rich_content, 
            }
            save_knowledge_as_json(OUTPUT_JSON_PATH, knowledge_entry)

    except Exception as e:
        print(f"âŒ Mainå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
